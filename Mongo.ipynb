{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '5.0.15', 'gitVersion': '935639beed3d0c19c2551c93854b831107c0b118', 'modules': ['enterprise'], 'allocator': 'tcmalloc', 'javascriptEngine': 'mozjs', 'sysInfo': 'deprecated', 'versionArray': [5, 0, 15, 0], 'bits': 64, 'debug': False, 'maxBsonObjectSize': 16777216, 'storageEngines': ['devnull', 'ephemeralForTest', 'inMemory', 'queryable_wt', 'wiredTiger'], 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1678270188, 3), 'signature': {'hash': b'\\x1e\\xf9|\"3\\x7f\\xd1\\xfe\\x87Z3\\x9b\\x00\\xd1\\x12\\xa6\\x17\\xff\\xb7\\xe4', 'keyId': 7161796363364073474}}, 'operationTime': Timestamp(1678270188, 3)}\n"
     ]
    }
   ],
   "source": [
    "conn_str = \"mongodb+srv://root:<password>@laplatforme.wb6fnma.mongodb.net/test\"\n",
    "client = pymongo.MongoClient(conn_str, serverSelectionTimeoutMS=5000)\n",
    "try:  \n",
    "    print(client.server_info())\n",
    "except Exception:\n",
    "    print(\"Unable to connect to the server.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discordBot', 'admin', 'local']\n"
     ]
    }
   ],
   "source": [
    "print(client.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = client[\"discordBot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intents']\n"
     ]
    }
   ],
   "source": [
    "print(mydb.list_collection_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycol = mydb[\"intents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('src/sample.json', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tag': 'greetings', 'patterns': ['hello', 'hey', 'hi', 'good day', 'greetings', 'how is it going ?'], 'responses': ['Hello !', 'Hey !', 'What can I do for you ?']}\n"
     ]
    }
   ],
   "source": [
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = mycol.insert_many(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObjectId('64085eed7bbf6f073a9bc863'), ObjectId('64085eed7bbf6f073a9bc864'), ObjectId('64085eed7bbf6f073a9bc865'), ObjectId('64085eed7bbf6f073a9bc866'), ObjectId('64085eed7bbf6f073a9bc867')]\n"
     ]
    }
   ],
   "source": [
    "# print(x.inserted_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "myquery = { \"tag\": \"accuracy\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc = mycol.find(myquery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('63ef4cc101d2e93524925d2f'), 'tag': 'accuracy', 'patterns': ['What is a reasonable way to compare the improvement in accuracy?', 'My accuracy changes throughout every epoc but the val_acc at the end of each epoc stays the same', \"Accuracy after selftraining didn't change\", '30% accuracy for training set, 80% for test set with a 0.3 split', 'Uncertainty in connection to explainability', 'Why are results without Transfer Learning better than with Transfer Learning?', 'Why are the Loss and Accuracy metrics not suitable?', 'Validation accuracy does not increase for binary classification using GNN', 'How to increase the Accuracy after Oversampling?', 'Evaluating optimal values for depth of tree', 'How to State The Confidence of Accuracy/Inaccuracy?', 'Balancing between accuracy and model overfitting'], 'responses': ['The tag is probably chosen really bad, but I have no idea what would be a better tag.', 'https://datascience.stackexchange.com/questions/94071/feature-extraction-with-mobilenet-visualization Hi, could you take a look here?', 'Out of curiosity, how to you retrain the model with the unlabeled samples? What is the target/loss?', 'Sounds fishy. But you provide not enough information to tell what could be wrong.', '@Peter I have added some information to the question. I fear that the 88% is for the padding. Would adding a Masking layer help?', 'I assume you meant $1-p_i$ instead of $1-a_i$, right?', '@Erwan yup thanks, fixed.', 'In the TL cases: did you only train the final layer? Generally, including your code would be useful.', 'I updated my question. So, I trained all the layers after `model = efnB0_model (input_img)`.', 'I agree this is unexpected. Could you share your dataset so we can reproduce those results and dig into their interpretation? Could you also share the other hyperparameters you used for training ? (optimizer, ...)', 'Maybe we could do this via chat? I updated my question regarding the hyperparameters.', '@etiennedm: I could share my colab notebook and data with you via mail?', 'Could you maybe provide the link to this chat?', 'No, I found it ;-)', 'sorry, here : https://chat.stackexchange.com/rooms/111498/discussion-between-tobitor-and-etiennedm', 'which book did you read? In fact no metric is a catch-all metric, that is why different metrics exist, there is not one super-metric. So it depends on the algorithm and what one wants to capture', 'Please, consider upvoting the answer if you found it useful, and marking it as correct if deemed so. Alternatively, please considering describing what the answer is lacking or why you think it is not correct, so that it can be improved.', 'what is the average performance?', '50% for training and validation. If training accuracy goes higher than 50%, validation and test accuracy drops which clearly means I am stuck at 50% optimal performance.', 'It depends on the kind of imbalance you have. It seems oversampling is not needed, but again it depends on amount of imbalance present. For example if 99,9%-0.01% then highly imbalanced and not much can be done', 'I used SMOTE, and I used this method because some class are very low compared to some other, for example the sum of class_3 is only 21, and the sum of class_1 is 168051.', 'This is weird. The accuracy on test set is highe then on the training set. What is the imbalance ratio ? How many samples in train and test set ?', 'Dear @AkashDubey, The dataset split is 80% 20%. For the imbalance ratio, I did not specify this argument to SMOTE, but after Oversampling, the number of samples in each class is same. NB. SMOTE is applied only to Training set.', '@Mimi Possible argument, please look at this : https://stackoverflow.com/questions/51464591/test-accuracy-is-greater-than-train-accuracy-what-to-do/51468429\\nAlso, this : https://stats.stackexchange.com/questions/59630/test-accuracy-higher-than-training-how-to-interpret', '@AkashDubey Thank you, but the linked question did not use any Oversampling method, as you can see in my question, the accuracy is low after using Oversampling.', \"@Mimi I don't think, this is because of the oversampling technique that you used. There is something inherently wrong with the model. What was the distribution of class labels before oversampling ? How many classes ? What modelling technique are you using ? Can you please add these details to the question.\", '@AkashDubey, I added some details.', 'Why would you want to oversample instead of modeling probabilities?', 'I think your y axis is labelled incorrectly. It should say log loss, not accuracy.', 'true, I was also wondering..   but the question itself( would) remain(s)?', 'This seems like it may be a regression problem involving continuous measurements rather than a classification problem involving discrete classes. By treating this as a classification problem, you treat any misclassifications as \"equally wrong\". Is it really the case that a prediction of 149 for a target of 150 is no better than a prediction of 1, or a prediction of 10,000? You may want to use the concept of error rather than accuracy - not whether you hit the target exactly or not, but *how far* from the target your prediction is.', 'I would be less concerned about the difference between train and test accuracy than about the train accuracy. What are your concerns about the difference between train and test accuracy?  // [Accuracy is more problematic than most people realize, even with perfect class balance.](https://stats.stackexchange.com/a/312787/247274)', 'Like Dave pointed out, is accuracy the right metric to use for your business problem? Is the cut off value for to determine the accuracy optimized for your business problem for both models? Looking to improve statistical performance in models you need to look at the features - get more/better features, do additional feature engineering, get more data if applicable.', \"I'm focusing on other aspects such as TN but using accuracy as a reference point for overfitting/ underfitting issues. Using XGB models, should I normalize the distribution of features or drop variables that has colinearity > .8? I read that XGB is quite robust to these issues internally and there is no need to do so.\", 'Mess with the decision threshold between the two classes. In my experience you can get a decent jump just based on not using .5.', 'Thanks a lot, Michael', 'say if the difference of TN between train and test is around 5%, should I be concerned about overfitting issues?']}\n",
      "{'_id': ObjectId('64085c1003917da3b8b6642e'), 'tag': 'accuracy', 'patterns': ['Compare two performance scores, each with a different baseline', 'Does it make sense to minimize AUC when using GBM with weights?', 'How to calculate specificity from accuracy and sensitivity', 'Accuracy of a polygon fitting algorithm', 'Measuring how good is one feature in predicting another', 'Calculating and reporting accuracy, sensitivity and specificity with multiple raters', 'How to calculate f-measure base of FPR, TPR, TNR, FNR & Accuracy?', 'Calculate classifier accuracy from per label accuracy', 'Breaking through an accuracy brickwall with my LSTM', 'Accuracy in multi-label classification', 'Keras accuracy not affected by any change in model', 'reconstruct a 2X2 confusion matrix (TP, TN, FP, FN) from Sensitivity and Specificity', 'AUC and accuracy interpretation', \"Calculation of accuracy (and Cohen's kappa) using sensitivity, specificity, positive and negative predictive values\", 'Is it possible to estimate accuracy, precision and recall with the given data?', 'Understanding Accuracy, Recall and IoU', 'RMSE to accuracy', 'Accuracy measure for model which sometimes produces no prediction', 'Low value on MAPE when using log CPI', 'Accuracy, F-Score, and Kappa Together', 'Model Validation Metric for correct count out of expected', 'Is it a problem if my model does not perform as well as the stupid model?', 'Accuracy, Recall (sensitivity), Specificity confusion', 'Determine test accuracy', 'Why is a 3-copula better than a 2-copula?', 'Understanding the idx column in h2o metrics', 'Why is there a difference between training and validation accuracy when both of them are pointing to the same subset?', 'Which metric to use for estimating accuracy of a climate model?'], 'responses': ['How about a propensity score adjustment?', \"Not sure whether this is really applicable to my problem here. I've never heard of it before, so forgive me, if I misunderstand something, but for which covariates would I calculate the propensity score? I already know with which likelihood a trial is assigned to a certain condition and therefore also the respective baseline level. Since the likelihoods are genuinely different, I expect the propensity scores also to be very different between conditions. How would the adjustment work, that you mentioned?\", \"So you already know the propensity score it sounds -- you know that one group has a 33.3% chance of receiving a condition, while the other has a 66.6% chance of being in that condition.  I wasn't sure if the two groups were somehow determined based on some set of covariates (this is usually what happens).  At any rate, you may still want to employ a propensity score approach -- this would allow you to either weight or adjust your data with the propensity score to offset the selection bias and correct for baseline differences -- this is a fairly standard approach in causal inference.\", 'Here are some good introductory papers for you that you will likely find helpful and directly applicable to your problem:  \\n1. http://www.tandfonline.com/doi/abs/10.1080/00273171.2011.568786#.Vbk0Ifn_XJM\\n2. http://jea.sagepub.com/content/34/1/66.refs\\n3. http://www.ncbi.nlm.nih.gov/pubmed/25773902', \"Thanks for the papers, @StatsStudent. They helped a lot getting things sorted. However, I'm still not sure about how to tackle my problem with it. Mostly, I have trouble drawing the analogies between subjects, treatments and outcome in the typical clinical setting and my subjects, conditions and outcome. As far as I understand, my 2 conditions are comparable to a treatment, just as my accuracy score is the outcome. However, what is usually referred to as subject, has to be something else, because each of mine, did both conditions. I edited my question, to add information on my study design.\", 'Or is it as simple as weighting the accuracies per condition with the inverse of respective baseline?', 'As I tried a few configurations which made some sense conceptually, but did not change the scores in a sensible way, I come to the conclusion that propensity adjustment is (most likely) not applicable to this case. Another issue is that the resulting scores,  are hard to interpret, because they are inverse accuracies, for which no real reference exists (values can move to infinity). @StatsStudent, I appreciate the suggestion, though.', 'I don\\'t agree with your assessment, but then again, I don\\'t know all the details of your project.  The resulting propensity scores are straightforward in terms of interpretation.  I\\'m not sure what you mean by the scores being \"inverse accuracies.\"  After adjusting, weighting, or stratifying on the estimated propensity scores, you would simply carry out your analyses in the manner you are used to and report the results as you normally would. Interpretation should not change -- the only thing you\\'d be doing with a propensity score analysis, is to reduce selection bias, which you clearly have.', \"Sorry, _inversed accuracies_ was pretty unclear. What I mean, is _inversed baselines_ . But still, the smaller the baseline, the higher the factor by which I correct my outcome variable, without upper limit. As I mentioned in the edit, I want to check which of the conditions produced more errors,  to check whether the effect in RT goes into the same direction, or whether a speed-acc-tradeoff is at hand. Unfortunately, the interpretation isn't that straightfoward to me, since the corrected outcome variables are even further apart from each other (acc: 0.7 vs. 0.9; corr. acc: 2.1 vs. 1.35)\", \"Auc makes sense for any model, regardless of how it's built as long as it's categorical.  It's just a criteria for trade off between false pos and true pos.\", \"AUC is a poor choice of metric for class imbalanced data. Precision-recall works better than sensitivity-specificity. I'm not sure how the weighting will affect this though, but probably just in how you will weight the importance of precision vs recall for your use case\", \"@Dan Sensitivity to class imbalance is an important distinction between PR and ROC curves, but I think that your explanation would greatly benefit from explaining why sensitivity to class imbalance is important. In my view, class imbalance is mostly an accident of data collection and data availability, so I prefer a measurement which isn't sensitive to that.\", \"@Sycorax heavy class imbalance is surely most often caused by the underlying distribution of what is being measured, not noise to be ignored. Consider a dataset of transactions, we want to classify fraudulent transactions. It may be that 99.999% of transactions are not fraudulent. What does that have to do with data collection or availability. It's simply a property of the problem being solved.\", \"You're probably right about fraud modeling. I don't know, I've never worked on that problem. But do you believe it's possible that there exists a problem where class imbalance *is* accidental? If so, that's a reason to prefer ROC analysis. That's all I'm saying -- use the tool that is appropriate to your task.\", \"@Sycorax if the classes are imbalance at say 60-40 I wouldn't even consider that imbalanced. If it's around 90-10 then I don't think it's an accident. If I read imbalanced classes, to me means closer to 90-10 or worse.\", 'Maybe the paper by Fawcett, [\"An Introduction to ROC analysis\"](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf), might be of help here.', 'I would bound $\\\\bar{R}$ by the total height and total width of your figure. Then you’re making your comparison to the naïve rectangle, somewhat like using $R^2$ to compare to naïve guessing of the pooled mean every time.', 'Why do you want to select ONLY one sensor?', \"It's theoretical situation. In fact it's not important how many sensors I'll use, but which of them deliver meaningul information.\", \"I know, this is rather far fetched, but have you ever found an appropriate solution to this problem? I'm running into the exact same issue and I was wondering how to solve it. Even a pointer would help.\", '@Dom42 , I invented my own method, for sensitivity, by adding all raters true positives and divided by the sum of all raters true positives and false negatives. Thus, if rater 1 had 11 TP and 3 FN, and rater 1 had 15 TP and 1 FN, I calculated (11+15)/(11+3+15+1)=86.7%.', \"@Dom42 \\nThis is not equal to the average, which would be (78.5+93.75)/2=86.1%.\\n\\nI have no justification for this, and I don't remember were I got the inspiration. But in my paper I did not have any citations for this method. I guess I just found it intuitive.\", 'Duplicate of [this](https://stats.stackexchange.com/questions/287112/how-can-calculatur-f-measure-base-of-fpr-tpr-tnr-fnr-and-acc) posted just a short time ago.', '@RichardHardy this is [ON HOLD].', 'Yes, and it is so for a reason. This one should be put on hold, too, because it is a copy of the other one which should have been edited instead of postding a new one. Please try to follow the standard practice here of editing what needs to be edited instead of posting it anew. You still can delete this one before it gets put on hold.', '@RichardHardy I tried to edit or delete the previous post, but I could not. Please delete your previous post.', 'I saw there was an attempt to edit the old one, with a decision pending. Was it someone else than you who did that?', '@RichardHardy Yes', 'Well, that is odd, but then I leave it here. Would you at least care to fix capital letters etc.?', '@Satwik Bhattamishra makes a valid point at the start of his answer, but not one that is necessarily universally applicable. Could you provide some insight into why you feel an aggregation of the per label results is preferred over direct calculation? Also, your calculation for label 11 does not align with the table presented (there are no fp, rather there are 4 tn in the table)', 'Thank you for your responses. I would like to know if this is possible, not preferable. I can calculate in both ways but have some constraints which are using per label accuracy. Also, for label `11`, `fp = columns[3].sum() - tp` which is `1`, correct?', 'Your FP for label 11 seems correct. What bothers me the most is your calculation of the true negatives. Notably, if you assign mat[2,2] as 0, then your overall accuracy is clearly 0. But your per-label accuracy will be non-negative for each label.', 'Since the overall accuracy only depends on the diagonal values and your per-label accuracy depends on other values (from which you are deriving the TN value), I do not think that it is possible to derive the overall accuracy from your measured per-label accuracy. Primarily because when you only have the per-label accuracy, there is no way to differentiate how much of it is due to the contribution of TP and how much of it is due to the contribution of TN.', 'I think you may have made a good point', 'With the values as floats, you can never be certain of how much weight any item contributes.', \"If you could update your answer to speak about this inability, I'll give it a look.\", 'Do you have any indicator that it is even possible to get lower error with the dataset you have?', 'Although this is an interesting thread, it does not really help in my situation, because that thread is about single-label classification. Like they state: \"we assign equal cost to false positives and false negatives\". This is exactly not, what we have here, because the definition only assigns cost to false negatives, not to false positives..', '[My answer at the duplicate thread](https://stats.stackexchange.com/a/312787/1352) applies equally well to multi-label classification.', 'Do you have 4 labels?', '@GermánAlfaro i believe so. initially as string, then formatted as integer, then to categorical (samples, 4) tensor', 'Is the loss decreasing?', 'only from epoch 1 to 2: 0.8073 to 0.5768 (and 0.5784 to 0.5783 for validation)', \"I notice that you haven't tried adjusting the learning rate. I suspect that could be the issue.\", \"How many training examples? It's also possible that your input variables are just not that informative.\", '@DezmondGoff see question edit - 271116 * 0.67 * 0.8 training samples', '@Sycorax see edit', 'Do you have the total $N$?', 'An AUC of 0.8 means that your model can successfully discriminate 80 % of your data based on a given threshold.', 'I think your interpretation boils down to the accuracy.', 'Interesting question. Looking at all possible combinations of TP, TN, FP, FN, each between 0 and 30, it appears like the answer is \"yes\", i.e., there are no two combinations with the same sensitivities, specificities, PPVs and NPVs but *different* accuracies. Of course, I still argue that you should not use accuracy to evaluate a classifier at all: [Why is accuracy not the best measure for assessing classification models?](https://stats.stackexchange.com/q/312780/1352) and [Is accuracy an improper scoring rule in a binary classification setting?](https://stats.stackexchange.com/q/359909/1352)', \"Thank you for your comment. I do understand accuracy limitations (that is why I asked about Cohen's kappa too), but in some fields it could be a commonly used value to communicate your results. Anyway, I tried to resolve a linear system with 4 equations, but I did not work as I intended (TP, TN, FP and FN were all equal to 0), probably I did something wrong\", 'What does \"What my classifier totally found\" mean? Is it number of positives, i.e. class F, your classifier found out of 19532?', '@gunes Yes, out of 19,532 instances, my classifier classified 11,120 to belong to class TS. Out of 11,120 of those instances, 6622 belonged to a the desired class F.', \"There should be two classes: F = positives, and F' = negatives. No class TS as I understand. So, 11,120 is classified as class F by your classifier. But, 6622 were really F. Isn't it?\", '@gunes Yes. 6622 is really F class, sorry for the confusion.', 'I am not into image-segmentation but I can tell you that accuracy is the proportion of all right predictions to the number of predictions. High recall AND accuracy is therefore very well possible.', 'What is IoU? How these are possible depends heavily on the base rate. See [Why is accuracy not the best measure for assessing classification models?](https://stats.stackexchange.com/q/312780/1352) The criticisms listed there apply also to recall (and I suspect to IoU).', 'I thought you should first know the four fundamental concepts: recall, precision, F measure and Jaccard index.', 'Is this a classification or regression problem?', 'Regression problem', 'Then what do you mean by “accuracy”?', 'The effectiveness (in percentage), is it wrong?', 'Is there some \"other\" category that such instances fall into? I.e., do you actually have a three-way classification problem (TRUE, FALSE, OTHER)?', 'Good question: no, there are only two categories (true/false). All the training, validation, and test data has a true/false classification.', 'Hm. What would a *correct* classification for something look like where your tool gives \"no prediction\"? Should it be \"error\"? If such observations do occur and cannot be filtered out beforehand, then it does look to me like you might have three classes. Can you explain in a little more detail?', 'The tool is saying: for this case, I make no prediction. (As an illustration, if it was in a medical domain, it might be for some inputs saying cancer or not cancer, and in some cases saying \"I don\\'t know - I won\\'t make a prediction\"). I\\'m wondering how to consider that when talking about accuracy (and similar metrics) and in comparisons to other models. Treating it as a third class might be a way forward (a class in which is never occupied by the data, or other models).', 'I see. But a correct classification would be one of the two \"real\" classes, correct?', \"Yes, that's right.\", 'They are not comparable: one is computed on the log scale and the other is not. If your goal is to minimize MAPE of CPI forecasts, then you have to back-transform the forecast from your log model and use that to compute MAPE on the untransformed CPI, regardless of if you transformed it to fit your model.', '@ChrisHaug: do you want to post your comment(s) as an answer? [Better to have a short answer than no answer at all.](https://stats.meta.stackexchange.com/a/5326/1352) Anyone who has a better answer can post it.', 'Hi Chris Haug! My goal is not to compare them. I want to run log(CPI) and see what MAPE I will have. So using MAPE on Log(CPI) do not work? So I have to back-transform it to normal CPI?', \"You should not compare different coefficients' values unless you make a special rescaling (e.g. correction for chance etc.).\", \"I don't understand your suggestion. All what I am trying to do is to look at different confusion matrix scores and understand the classification performance. In this respect, I would like to know what extra will f-score contribute to my understanding above accuracy and kappa.\", '[Why is accuracy not the best measure for assessing classification models?](https://stats.stackexchange.com/q/312780/1352)\\n[Is accuracy an improper scoring rule in a binary classification setting?](https://stats.stackexchange.com/q/359909/1352)\\n[Classification probability threshold](https://stats.stackexchange.com/q/312119/1352)\\nThe same problems apply to your KPI, and indeed to all evaluation metrics that rely on hard classifications. Instead, use probabilistic classifications, and evaluate these using [proper scoring rules](https://stats.stackexchange.com/tags/scoring-rules/info).', \"FWIW, I'm not advocating this method.. I just want to know if it has a name\", 'If you have an imbalance, accuracy is probably not a good number to use. You should look closer at precision and recall. I would suggest MCMC resampling of the lesser-known class.', \"I downsampled the majority outcome. You're suggesting upsampling of the minority outcome using MCMC. SPSS Modeler software doesn't have MC so I just used its vanilla upsampling of the minority outcome. Remarkably I got 98.5%!! Even higher than the baseline of 95%. Thanks!  But here we are talking about balancing the target. What about the issue that my baseline before/after feature engineering is 95% and so are any models I throw at it. Everything is always 95%. Why is this? And why did upsampling work better than downsampling? p.s. You should write an answer so I can mark it as correct :)\", 'p.s. my flow is clean->feature engineer->baseline->balance->model. Not sure if this matters.', 'One other question. Why do we balance the target but not the inputs?', '[Class imbalance is not such a problem when you use proper statistical methods, and balancing the classes won’t solve a non-problem.](https://stats.stackexchange.com/questions/357466/are-unbalanced-datasets-problematic-and-how-does-oversampling-purport-to-he) // Yes, it’s a problem if your fancy modeling cannot beat the silly model. In fact, $R^2$ in linear regression is, in some sense, measuring the amount by which you beat a silly model.', \"@Dave Thanks :) Btw, What about the issue that my baseline before/after feature engineering is 95% and so are all flavours of models I throw at it. Everything is always 95%. I thought that even regardless of whether it's before or afterfeature engineering the baseline accuracy and model accuracy should always be different.  Is it because in my case the imbalance is so high that the models just automatically predict/classify based on the majority outcome?\", '@LarsvanderLaan Thank you. `\"...more useful to look at the conditional odds ratios or relative risk relations between the outcome and baseline variables\"` means issues like multicollinearity? And `\"...identify high risk regions of the covariate space\"`, means inputs that are not significant?  --- plotting a histogram it was flat with everything at 5110 (sample size).   image here >> https://i.imgur.com/7Yt7BCh.png', 'Logistic Regression screenshot >> https://i.imgur.com/hJfya7g.png', \"Goal is to predict probability of having a stroke. Target variable is flag/binary [0,1]. I'm currently using C5, CART, KNN. For my features and target upsampling, those 3 seem to have the highest accuracy. Amazingly, I'm forced to use SPSS Modeler at university instead of Python.\", \"I mistakenly ran a histogram on the CART model before that's why it was flat. Here is a histogram for the logistic regression model. Before feature engineering and before balancing. https://i.imgur.com/cysL5Xq.png\", 'Do not use accuracy. [This thread](https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models) has already been mentioned.', 'Wikipedia article on [confusion matrix](https://en.m.wikipedia.org/wiki/Confusion_matrix) is good as a reference.', '@RogerVadim Thanks my friend! I am always referring to this page...but would appreciate it if you look into my question which is about \"relationship between accuracy and other metrics and confusion about it\"', \"If anyone has any idea, I'd love to hear\", 'The results will be *different.* When you fit a 3-copula you are estimating the full 3-variate distribution of $(X,Y,Z).$ A trio of (marginal) 2-copulas usually does not determine that 3-variate distribution.  Whether the results are \"superior\" or not then depends on what you need to estimate.  If your interests lie in the 2-variate marginals and you don\\'t require perfect mathematical consistency among them, the three 2-copula solution might be better.', 'Maybe more suitable to stackoverflow.', \"You're asking about the behavior of a model, but you haven't told us what the model is, or how it's trained. More to the point, does your network use dropout, batch norm, or other regularization techniques that behave differently during training and testing?\", '@Sycorax updated as you suggested', 'Thanks for doing that. We\\'re making progress. Note that if you use the train accuracy that you get from each mini-batch as you update the weights, then the accuracy you get from the first mini-batch in an epoch will be \"stale\" compared to the accuracy from the last mini-batch. How are you measuring accuracy?', 'The training accuracy was measured using the latest updated model at the end of each epoch. I guess that is how TensorFlow calculates the accuracy by default.', 'Firstly, you might want to look into \"downscaling\" methods.  If a model run is more accurate on a daily basis, it is probably just random chance.  If you want to determine which model is working better, you might want to see how accurate it is on timescales long enough for the \"weather\" to average out, e.g. seasonal averages.  More skillfull models will get the statistical properties right (e.g. means and variances), they won\\'t be good at predicting the noise (the weather on a particular day).']}\n"
     ]
    }
   ],
   "source": [
    "for x in mydoc:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
